\chapter{EXPERIMENTS}
\label{expr}
The experiments conducted to evaluate the functionality and performance of
the FFVM emulate network switch behavior using multiple threading architectures.
To drive the FFVM, a high level network application language called Steve
\cite{steve}, developed in collaboration with a fellow research associate,
provided the hosted applications. Each experiment is comprised of a FFVM
driver, which creates an instance of a virtual switch, and the compiled network
application. Input for the experiments, network traffic, is generated by an
external application that transmits the contents of a packet capture (PCAP)
file. PCAP files contain live network data that has been formatted for usage
by 3rd party libraries, such as libpcap \cite{libpcap}. The retransmission of
the capture file simulates a steady flow of input for each experiment to
evaluate its performance. The traffic is ``framed'' over TCP sockets to
emulate raw Ethernet frames that would be received at the lowest network
protocol layer. A small protocol header occupies the first 4 bytes of each
frame to denote the length of the proceeding data. In the following sections,
the goals for each experiment are explained along with the different threading
architectures implemented, and lastly the performance metrics collected during
each trial.

\section{L1 Receive - Endpoint}
\label{expr:endpoint}
The baseline test for the FFVM is an endpoint application, where a simple
server is constructed that accepts a single connection and reports the
receive rate. This test evaluates the maximum rate that data can be received
and processed by an application. Below is a simplified listing of the driver
that creates an instance of a virtual switch and executes the ``endpoint''
application.

\begin{lstlisting}
// Build a server socket that will accept network connections.
Ipv4_socket_address addr(Ipv4_address::any(), 5000);
Ipv4_stream_socket server(addr);

// Pre-create all standard ports.
Port_eth_tcp port1(1);

// Configure the dataplanes ports before loading applications.
ff::Dataplane dp = "dp1";
dp.add_port(&port1);
dp.load_application("apps/endpoint.app");
dp.up();

while (running) {
	poll(server, port1);

	if (server.can_read())
		accept_new_connection(server);

	if (port1.can_read()) {
		ff::Context cxt;
		port1.receive(cxt);

		dp.get_application()->process(cxt);

		if (cxt.has_output_port())
			cxt.output_port()->send(cxt);
	}
}
\end{lstlisting}


\section{L2 Forwarding - Wire}
\label{expr:wire}
The second experiment conducted creates an L2 wire that behaves similarly
to the modified DPDK example discussed in Chapter \ref{hardware}. Setup for
this experiment extends the ``endpoint'' driver by adding an additional TCP
Ethernet port, and utilizes slightly more functional application named
``wire.app''. During pipeline processing, each context's output port is set
to the ``opposite'' of the input port. The wire driver listing is shown below.

\begin{lstlisting}
// Build a server socket that will accept network connections.
Ipv4_socket_address addr(Ipv4_address::any(), 5000);
Ipv4_stream_socket server(addr);

// Pre-create all standard ports.
Port_eth_tcp port1(1);
Port_eth_tcp port2(2);

// Configure the dataplanes ports before loading applications.
ff::Dataplane dp = "dp1";
dp.add_port(&port1);
dp.add_port(&port2);
dp.load_application("apps/wire.app");
dp.up();

while (running) {
	poll(server, port1, port2);

	if (server.can_read())
		accept_new_connection(server);

	if (port1.can_read()) {
		ff::Context cxt;
		port1.receive(cxt);

		dp.get_application()->process(cxt);

		if (cxt.has_output_port())
			cxt.output_port()->send(cxt);
	}

  if (port2.can_read()) {
		ff::Context cxt;
		port2.receive(cxt);

		dp.get_application()->process(cxt);

		if (cxt.has_output_port())
			cxt.output_port()->send(cxt);
	}
}
\end{lstlisting}

Different threading models were considered in the wire implementation, and shed
light on optimizations that could be made to the shared resources provided in
the FFVM. Further explanations of the different threading models used are found
in Section \ref{expr:models}. In a multi-threaded architecture the routines
ports execute are defined as free functions, where each port receives packets
and after pipeline processing rather than directly sending them out the
designated output port, the context is copied into a send queue tied to each
port. A simplified listing of the multithreaded wire driver is shown below.

\begin{lstlisting}
// Global resources.
ff::Port_eth_tcp ports[2];
ff::Queue_concurrent<Context> send_queue[2];

void* port_work(void* arg){
  // Get the thread id.
  int id = *((int*)arg);
  // Run while the port is connected.
  while (ports[id].up()){
    // Get a free buffer from the pool and receive a packet.
    ff::Context cxt;
    ports[id].receive(cxt);
    // Process.
    dp.get_application()->process(cxt);
    // Place buffer id in the output port's send queue.
    if (int out_id = cxt.output_port())
      send_queue[out_id].push(buf);
    // Send packets queued on this port.
    if (ff::Buffer out_buf = send_queue[id].pop()) {
      ports[id].send(cxt);
    }
  }
  return 0;
}

int main(int argc, char** argv){
  // Build a server socket that will accept network connections.
  Ipv4_socket_address addr(Ipv4_address::any(), 5000);
  Ipv4_stream_socket server(addr);

  // Pre-create all standard ports.
  ports[0] = ff::Port_eth_tcp(1);
  ports[1] = ff::Port_eth_tcp(2);

  // Configure the dataplanes ports before loading applications.
  ff::Dataplane dp = "dp1";
  dp.add_port(&ports[0]);
  dp.add_port(&ports[1]);
  dp.load_application("apps/wire.app");
  dp.up();

  while (running) {
  	poll(server);
  	if (server.can_read())
  		accept_new_connection_and_launch_thread(server);
  }
  return 0;
}
\end{lstlisting}

In this setup the only resource that can cause concurrency issues is the global
send queue associated with each port. The initial implementation of the FFVM
concurrent queue utilized mutexes to control read/write access from multiple
threads, however the bottlekneck created by the locking mechanism drastically
reduced performance. To provide a concurrent lockfree queue, an implementation
was integrated from the Boost C++ libraries \cite{boost}. Performance improved
greatly with the usage of the lockfree queue, yet there were still issues with
memory usage. Each port creates a local context and pushes a copy into the
send queue for the output port, and context is then copied again before being
sent from the designated output port. This warranted the need for a reusable
object pool, where contexts and packet data would reside in a pre-allocated
system buffer. Rather than copying the context to and from the send queues, the
buffer id is enqueued. Ports are able to access the context and packet through
the buffer pool using the buffer id as the offset into the pool's underlying
data store. The final optimzation added in this experiment was to have ports
cache a local store of processed buffer ids and copy the contents of the local
store when it is ``full''. Perfomance increased greatly over the previous driver
implementation by utilizing re-usable packet buffers and the local store of
buffer ids. A listing for the final wire driver is show below.

\begin{lstlisting}
// Global resources.
ff::Port_eth_tcp ports[2];
ff::Object_pool<ff::Buffer> buffer_pool;
ff::Queue_concurrent<int> send_queue[2];

void* port_work(void* arg){
  // Get the thread id.
  int id = *((int*)arg);
  // Create local stores for processed buffer ids.
  int local_size_max = 1024;
  int local_size = 0;
  std::vector<int> local_in, local_out;
  // Run while the port is connected.
  while (ports[id].up()){
    // Get a free buffer from the pool and receive a packet.
    ff::Buffer buf = buffer_pool::alloc();
    ports[id].receive(buf.context());
    // Process.
    dp.get_application()->process(buf.context());
    // Cache the buffer id if the output port is set.
    if (int out_id = buf.context().output_port()){
      local_in[local_size++].push_back(buf.id());
      // When the local store is 'full', push to output port's send queue.
      if (local_size == local_size_max) {
        send_queue[out_id].push(local_in);
        local_in.clear();
        local_size = 0;
      }
    }
    // Send packets queued on this port.
    if (local_out = send_queue[id].pop()) {
      for (int idx : local_out) {
        ports[id].send(buffer_pool[idx].context());
        buffer_pool.dealloc(idx);
      }
    }
  }
  return 0;
}
\end{lstlisting}


\section{Threading Models}
\label{expr:models}
Given the modular nature of the FFVM, the threading architecture of a virtual
switch is incredibly flexible. In a single threaded architecture (STA), all
component behavior, such as port work routines and application pipeline
execution, is defined and executed by the main thread in the driver. To elevate
an application to utilize multiple threads, the behavior for the desired
components can be defined as a free function in the driver and assign that
work stub to a thread.

\subsection{Single Threaded}
\label{expr:models-single}
As a baseline implementation for most applications, an STA driver will service
the FFVM and the server. The server port and FFVM ports can be tied to a polling
mechanism to switch between events occuring on multiple port objects. This
architecture places the port and application pipeline processing on an even
playing field. A process diagram for the STA wire driver is show in Figure
\ref{wire_sta_diag}.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=.8]{wire_sta_diag}
  \caption{The Freeflow STA wire state machine.}
  \label{wire_sta_diag}
\end{figure}

\subsection{Thread Per Port}
\label{expr:models-port}
In a thread per port (TPP) model, the server and FFVM ports are separated to
allow packet I/O to operate asynchroneously. The main thread from the driver
handles the server, and spawns a new thread to drive a port object when a
successful connection is established.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=.75]{wire_tpp_diag}
  \caption{The Freeflow TPP wire state machine.}
  \label{wire_tpp_diag}
\end{figure}

\section{Results}
\label{expr:results}
The evaluation of the experiments conducted focuses on the measured performance
of each application with respect to throughput and bandwidth. Throughput is
defined as the number of packets per second, Pps, sent or received and
bandwidth as the amount of data (in Gigabits) per second, Gbps, sent or
received.

\subsection{Endpoint}
\label{expr:results:endpoint}
In the endpoint test, the average send receive rates for throughput and
bandwidth between a baseline (Flowcap to Flowcap) and the Freeflow (Flowcap to
Endpoint) implementations are measured. This highlights any negative impacts on
performance that are incurred using a Freeflow application. Table
\ref{endpoint_performance} lists the observed performance for both of the
testing scenarios.

\begin{table}[h!]
  \centering
  \begin{tabular}{| C{3.5 cm} | C{2.5cm} | C{2.5cm} | C{2.5cm} | C{2.5cm} |}
    \hline
    Implementation & Pps RX & Pps TX & Gbps RX & Gbps TX \\ [0.5ex]
    \hline
    Baseline & 2061814 & 2058960  & 10.167 & 10.152\\
    \hline
    Freeflow & 650864 & 707885 & 3.209 & 3.490 \\
    \hline
  \end{tabular}
  \caption{Freeflow endpoint driver and traffic generator performance metrics
  with respect to throughput and bandwidth.}
  \label{endpoint_performance}
\end{table}

The simple end-to-end connection created by the two Flowcap applications in the
baseline example shows that the sink is able to process data faster than the
source can send. Thus the rate at which packets can be received is bounded
by the rate at which they can be sent. Utilizing the Freeflow Endpoint
application introduces some overhead, attributed to the time it takes a packet
to be processed by the application pipeline. These findings are shown in Table
\ref{endpoint_cost}, listing the cost in overhead between the source and sink
throughput and bandwidth rates.

\begin{table}[h!]
  \centering
  \begin{tabular}{| C{3.5cm} | C{3cm} | C{3cm} |}
    \hline
    Implementation & Throughput $\Delta$ & Bandwidth $\Delta$ \\ [0.5ex]
    \hline
    Baseline & 0.139\% & 0.148\% \\
    \hline
    Freeflow & -8.055\% & -8.052\% \\
    \hline
  \end{tabular}
  \caption{Flowcap and Freeflow endpoint overhead cost with respect to
  throughput, Pps, and bandwidth, Gbps.}
  \label{endpoint_cost}
\end{table}

\subsection{Wire}
\label{expr:results:wire}
The Freeflow wire test evaluates the performance within the FFVM running an L2
forwarding application. For the purpose of this experiment the baseline case,
the STA wire driver, is compared against the performance of the multiple
versions of the TPP wire driver. Each TPP driver version corresponds to
different shared resource usages explored during its implementation. Version 1
uses locked queues and copies the context on each push/pop, while Version 2
utilizes lockfree queues. In Version 3, lockfree queues hold containers of
buffer ids that are generated by each thread lcoally, eliminating the copying of
contexts. Table \ref{wire_performance} lists the observed performance for all
four of the Freeflow wire implementations tested.

\begin{table}[h]
  \centering
  \begin{tabular}{| C{2.5cm} | C{2.5cm} | C{2.5cm} | C{2.5cm} | C{2.5cm} |}
   \hline
   Architecture & Pps RX & Pps TX & Gbps RX & Gbps TX \\ [0.5ex]
   \hline
   STA & 504829 & 504829 & 1.861 & 1.861 \\
   \hline
   TPP (v1) & 272515 & 121233 & 0.744 & 0.743 \\
   \hline
   TPP (v2) & 627801 & 627801 & 3.025 & 3.025 \\
   \hline
   TPP (v3) & 836129 & 950435 & 2.947 & 3.559 \\ [1ex]
   \hline\hline
   Speedup & 1.66 & 1.88 & 1.58 & 1.91 \\ [1ex]
   \hline
\end{tabular}
\caption{Freeflow wire driver performance metrics, including relative speedup.}
\label{wire_performance}
\end{table}

With each version of the TPP wire driver, the resulting performance of the FFVM
hosted application changes. The initial TPP version 1 resulted in a major loss
with respect to packet throughput, but was fixed by the use of lockfree queue
structures in version 2. In version 3, more optimizations were implemented and
gave way to the transmission rate exceeding the reception rate in terms of
throughput and bandwidth. These findings are listed in Table
\ref{wire_penalties}.

\begin{table}[h]
  \centering
  \begin{tabular}{| C{2.5cm} | C{2.5cm} | C{2.5cm} |}
    \hline
    Architecture & Throughput & Bandwidth \\ [0.5ex]
    \hline
    STA &  0.0\% & 0.0\% \\
    \hline
    TPP (v1) & 55.5\% & 0.1\%\\
    \hline
    TPP (v2) & 0.0\% & 0.0\% \\
    \hline
    TPP (v3) & -13.7\%  & -20.8\%\\ [1ex]
    \hline
  \end{tabular}
  \caption{Freeflow wire driver system overhead penalties with respect to
  throughput, Pps, and bandwidth, Gbps.}
  \label{wire_penalties}
\end{table}
